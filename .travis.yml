---
sudo: required
dist: bionic
env:
  global:
  # auto vagrant installation
  - VAGRANT_CURRENT_VERSION="$(curl -s https://checkpoint-api.hashicorp.com/v1/check/vagrant | jq -r -M '.current_version')"
  # auto vagrant installation
  - VAGRANT_VERSION="2.2.9"
  - KUBECTL_VERSION=1.18.3
  - KUBERNETES_VERSION=1.18.3
  - MINIKUBE_VERSION=1.8.3
  - CHANGE_MINIKUBE_NONE_USER=true #(bool) automatically change ownership of ~/.minikube to the value of $SUDO_USER https://minikube.sigs.k8s.io/docs/handbook/config/
  - MINIKUBE_WANTREPORTERRORPROMPT=false
  - MINIKUBE_WANTUPDATENOTIFICATION=false #(bool) sets whether the user wants an update notification for new minikube versions https://minikube.sigs.k8s.io/docs/handbook/config
  - MINIKUBE_HOME=$HOME #(string) sets the path for the .minikube directory that minikube uses for state/configuration. Please note: this is used only by minikube https://minikube.sigs.k8s.io/docs/handbook/config
  - KUBECONFIG=$HOME/.kube/config

notifications:
  slack:
    on_failure: always

#https://microk8s.io/#get-started
#https://microk8s.io/docs
#https://istio.io/docs/setup/platform-setup/microk8s/
fleet_script_microk8s_linkerd_knative_tasks : &fleet_script_microk8s_linkerd_knative_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - sudo snap install microk8s --classic --channel=1.18/stable
          - sudo usermod -a -G microk8s $USER #add your current user to the group and gain access to the .kube caching directory
          # - sudo chown -f -R $USER ~/.kube
          # - su - $USER # re-enter the session for the group update to take place
          - sudo microk8s status --wait-ready #Check the status
          - sudo microk8s kubectl get nodes
          - sudo microk8s kubectl get services
          # - alias kubectl='microk8s kubectl' #add an alias (append to ~/.bash_aliases)
          - sudo microk8s  kubectl create deployment my-dep --image=busybox
          - sudo microk8s kubectl get pods #Check the status
          - sudo microk8s status --wait-ready
          - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress linkerd jaeger knative kubeflow metrics-server prometheus rbac registry storage#Use add-ons linkerd
          # - microk8s stop
          # - microk8s start
          # - sudo microk8s enable dns dashboard registry #Turn on standard services
          # - watch microk8s.kubectl get all --all-namespaces #check deployment progress
          - echo "=========================================================================================="
          - sudo microk8s kubectl version
          - sudo microk8s kubectl version --client #the version of the client
          - sudo microk8s kubectl cluster-info
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if sudo microk8s kubectl get pods --namespace=kube-system | grep Running ; then
                break
              fi
              sleep 2
            done
          - sudo microk8s kubectl get all --all-namespaces
          - sudo microk8s kubectl run nginx --image nginx --replicas 3
          - sudo microk8s kubectl get all --all-namespaces

#https://microk8s.io/#get-started
#https://microk8s.io/docs
#https://istio.io/docs/setup/platform-setup/microk8s/
fleet_script_microk8s_linkerd_tasks : &fleet_script_microk8s_linkerd_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - sudo snap install microk8s --classic --channel=1.18/stable
          - sudo usermod -a -G microk8s $USER #add your current user to the group and gain access to the .kube caching directory
          # - sudo chown -f -R $USER ~/.kube
          # - su - $USER # re-enter the session for the group update to take place
          - sudo microk8s status --wait-ready #Check the status
          - sudo microk8s kubectl get nodes
          - sudo microk8s kubectl get services
          # - alias kubectl='microk8s kubectl' #add an alias (append to ~/.bash_aliases)
          - sudo microk8s  kubectl create deployment my-dep --image=busybox
          - sudo microk8s kubectl get pods #Check the status
          - sudo microk8s status --wait-ready
          - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress linkerd jaeger knative kubeflow metrics-server prometheus rbac registry storage#Use add-ons linkerd
          # - microk8s stop
          # - microk8s start
          # - sudo microk8s enable dns dashboard registry #Turn on standard services
          # - watch microk8s.kubectl get all --all-namespaces #check deployment progress
          - echo "=========================================================================================="
          - sudo microk8s kubectl version
          - sudo microk8s kubectl version --client #the version of the client
          - sudo microk8s kubectl cluster-info
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if sudo microk8s kubectl get pods --namespace=kube-system | grep Running ; then
                break
              fi
              sleep 2
            done
          - sudo microk8s kubectl get all --all-namespaces
          - sudo microk8s kubectl run nginx --image nginx --replicas 3
          - sudo microk8s kubectl get all --all-namespaces


fleet_script_microk8s_istio_tasks : &fleet_script_microk8s_istio_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          # - sudo snap install microk8s --classic --channel=1.18/stable
          - sudo usermod -a -G microk8s $USER #add your current user to the group and gain access to the .kube caching directory
          # - sudo chown -f -R $USER ~/.kube
          # - su - $USER # re-enter the session for the group update to take place
          - sudo microk8s status --wait-ready #Check the status
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              watch -d sudo microk8s kubectl get all --all-namespaces
              sleep 2
            done
          - sudo microk8s kubectl get nodes
          - sudo microk8s kubectl get services
          # - alias kubectl='microk8s kubectl' #add an alias (append to ~/.bash_aliases)
          # - sudo microk8s  kubectl create deployment my-dep --image=busybox
          - sudo microk8s kubectl get pods #Check the status
          - yes | sudo microk8s enable istio     #Enforce mutual TLS authentication (https://bit.ly/2KB4j04) between sidecars? If unsure, choose N. (y/N):
          #Istio needs to inject sidecars to the pods of your deployment
          #In microk8s auto-injection is supported  label the namespace you will be using with istion-injection=enabled
          - sudo microk8s kubectl label namespace default istio-injection=enabled
          - wget https://raw.githubusercontent.com/istio/istio/release-1.0/samples/bookinfo/platform/kube/bookinfo.yaml  #the bookinfo example from the v1.0 Istio release
          - sudo microk8s.kubectl create -f bookinfo.yaml
          #reach the services using the ClusterIP they have
          #for example get to the productpage in the above example by pointing our browser to 10.152.183.59:9080
          - sudo microk8s kubectl get svc
          - wget https://raw.githubusercontent.com/istio/istio/release-1.0/samples/bookinfo/networking/bookinfo-gateway.yaml #exposing the services via NodePort:
          - sudo microk8s kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}' 31380 #get to the productpage through ingress
          - curl http://localhost:31380/productpage
          # - nc http://localhost:31380/productpage
          - sudo microk8s kubectl -n istio-system get svc grafana
          - sudo microk8s kubectl -n istio-system get svc prometheus
          - sudo microk8s kubectl -n istio-system get service/jaeger-query
          - sudo microk8s kubectl -n istio-system get servicegraph
          # -  sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress istio jaeger knative kubeflow metallb metrics-server prometheus rbac registry storage #Use add-ons istio
          # - microk8s stop
          # - microk8s start
          # - sudo microk8s enable dns dashboard registry #Turn on standard services
          # - watch microk8s.kubectl get all --all-namespaces #check deployment progress
          - echo "=========================================================================================="
          - sudo microk8s kubectl version
          - sudo microk8s kubectl version --client #the version of the client
          - sudo microk8s kubectl cluster-info
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if sudo microk8s kubectl get pods --namespace=kube-system | grep Running ; then
                break
              fi
              sleep 2
            done
          - sudo microk8s kubectl get all --all-namespaces
          - sudo microk8s kubectl run nginx --image nginx --replicas 3
          - sudo microk8s kubectl get all --all-namespaces

fleet_script_microk8s_cilium_tasks : &fleet_script_microk8s_cilium_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - sudo snap install microk8s --classic --channel=1.18/stable
          - sudo usermod -a -G microk8s $USER #add your current user to the group and gain access to the .kube caching directory
          # - sudo chown -f -R $USER ~/.kube
          # - su - $USER # re-enter the session for the group update to take place
          - sudo microk8s status --wait-ready #Check the status
          - sudo microk8s kubectl get nodes
          - sudo microk8s kubectl get services
          # - alias kubectl='microk8s kubectl' #add an alias (append to ~/.bash_aliases)
          - sudo microk8s  kubectl create deployment my-dep --image=busybox
          - sudo microk8s kubectl get pods #Check the status
          - sudo microk8s status --wait-ready
          - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress jaeger kubeflow metallb metrics-server prometheus rbac registry storage #Use add-ons istio
          # - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress istio jaeger knative kubeflow metallb metrics-server prometheus rbac registry storage #Use add-ons istio
          # - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress linkerd jaeger knative kubeflow metrics-server prometheus rbac registry storage#Use add-ons linkerd
          # - microk8s stop
          # - microk8s start
          # - sudo microk8s enable dns dashboard registry #Turn on standard services
          # - watch microk8s.kubectl get all --all-namespaces #check deployment progress
          - echo "=========================================================================================="
          - sudo microk8s kubectl version
          - sudo microk8s kubectl version --client #the version of the client
          - sudo microk8s kubectl cluster-info
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if sudo microk8s kubectl get pods --namespace=kube-system | grep Running ; then
                break
              fi
              sleep 2
            done
          - sudo microk8s kubectl get all --all-namespaces
          - sudo microk8s kubectl run nginx --image nginx --replicas 3
          - sudo microk8s kubectl get all --all-namespaces

#https://microk8s.io/docs/install-alternatives
fleet_script_microk8s_win_tasks : &fleet_script_microk8s_win_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - choco install wget
          - wget https://github.com/ubuntu/microk8s/releases/download/installer-v2.0.0/microk8s-installer.exe
          - dir
          #PowerShell
          # - pwsh -c 'if(-not (Get-Module -ListAvailable -Name BadModuleName)){ exit 1 }' && echo yes || echo no #check if Powershell has a certain module installed
          # - client = new-object System.Net.WebClient
          # - pwsh client.DownloadFile("https://github.com/ubuntu/microk8s/releases/download/installer-v2.0.0/microk8s-installer.exe","C:\tmp\microk8s-installer.exe")
          # #PowerShell
          # - pwsh -c 'if(-not (Get-Module -ListAvailable -Name Invoke-WebRequest)){ exit 1 }' && echo yes || echo no #check if Powershell has a certain module installed
          # - pwsh Invoke-WebRequest -OutFile microk8s-installer.exe https://github.com/ubuntu/microk8s/releases/download/installer-v2.0.0/microk8s-installer.exe #PowerShell >= 3.0
          # - iwr -outf 1_microk8s-installer.exe https://github.com/ubuntu/microk8s/releases/download/installer-v2.0.0/microk8s-installer.exe #PowerShell >= 3.0
          # - bitsadmin /transfer myDownloadJob /download /priority normal http://downloadsrv/10mb.zip c:\10mb.zip
          # - bitsadmin /transfer myDownloadJob /download /priority normal https://github.com/ubuntu/microk8s/releases/download/installer-v2.0.0/microk8s-installer.exe c:\microk8s-installer.exe
          # - microk8s status --wait-ready
          # - microk8s enable dashboard dns registry istio # enable add-ons, services
          # - microk8s kubectl get all --all-namespaces
          # - microk8s dashboard-proxy

#https://microk8s.io/docs/install-alternatives
fleet_script_microk8s_macos_tasks : &fleet_script_microk8s_macos_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)" #Install Homebrew
          # - microk8s install
          # - microk8s status --wait-ready
          # - microk8s enable dashboard dns registry istio # enable add-ons, services
          # - microk8s kubectl get all --all-namespaces
          # - microk8s dashboard-proxy

fleet_script_microk8s_tasks : &fleet_script_microk8s_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          # - sudo snap install microk8s --classic --channel=1.18/stable
          - sudo usermod -a -G microk8s $USER #add your current user to the group and gain access to the .kube caching directory
          # - sudo chown -f -R $USER ~/.kube
          # - su - $USER # re-enter the session for the group update to take place
          - sudo microk8s status --wait-ready #Check the status
          - sudo microk8s kubectl get nodes
          - sudo microk8s kubectl get services
          # - alias kubectl='microk8s kubectl' #add an alias (append to ~/.bash_aliases)
          - sudo microk8s  kubectl create deployment my-dep --image=busybox
          - sudo microk8s kubectl get pods #Check the status
          - sudo microk8s status --wait-ready
          - yes | sudo microk8s enable istio     #Enforce mutual TLS authentication (https://bit.ly/2KB4j04) between sidecars? If unsure, choose N. (y/N):
          #kubeflow hanging
          # Enabling MetalLB
          # Enter the IP address range (e.g., 10.64.140.43-10.64.140.49):
          # - echo"10.64.140.43-10.64.140.49" | sudo microk8s enable metallb
          - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress jaeger knative metrics-server prometheus rbac registry storage #Use add-ons,services
          # - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress jaeger knative metallb metrics-server prometheus rbac registry storage #Use add-ons,services
          # - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress jaeger knative kubeflow metallb metrics-server prometheus rbac registry storage #Use add-ons,services
          # - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress istio jaeger knative kubeflow metallb metrics-server prometheus rbac registry storage #Use add-ons istio
          # - sudo microk8s enable dns cilium dashboard fluentd helm helm3 ingress linkerd jaeger knative kubeflow metrics-server prometheus rbac registry storage#Use add-ons linkerd
          # - microk8s stop
          # - microk8s start
          # - sudo microk8s enable dns dashboard registry #Turn on standard services
          # - watch microk8s.kubectl get all --all-namespaces #check deployment progress
          - echo "=========================================================================================="
          - sudo microk8s kubectl version
          - sudo microk8s kubectl version --client #the version of the client
          - sudo microk8s kubectl cluster-info
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if sudo microk8s kubectl get pods --namespace=kube-system | grep Running ; then
                break
              fi
              sleep 2
            done
          - sudo microk8s kubectl get all --all-namespaces
          - sudo microk8s kubectl run nginx --image nginx --replicas 3
          - sudo microk8s kubectl get all --all-namespaces


#https://istio.io/docs/setup/platform-setup/gardener/
#https://github.com/gardener/gardener/blob/master/docs/development/local_setup.md
fleet_script_minikube_gardener_tasks : &fleet_script_minikube_gardener_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - sudo snap install helm --classic
          - sudo apt-get -qqy install openvpn
          - egrep -c '(vmx|svm)' /proc/cpuinfo | echo "virtualization is  supported" | echo "virtualization is not supported"
          - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
          - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
          - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
          - mkdir -p $HOME/.kube $HOME/.minikube
          - touch $KUBECONFIG
          - sudo snap install kubectl --classic && kubectl version –client
          - echo "=====================Local Gardener setup Using minikube====================================================================="
          - minikube config set embed-certs true
          - minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION --embed-certs #  `--embed-certs` can be omitted if minikube has already been set to create self-contained kubeconfig files.
          - make dev-setup #point your KUBECONFIG environment variable to the local cluster you created in the previous step and run
          - kubectl apply -f example/10-secret-internal-domain-unmanaged.yaml
          - make start-apiserver
          - make start-controller-manager
          - make start-scheduler
          - make start-gardenlet
          - kubectl get shoots
          - echo "=========================================================================================="
          # - sudo minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION #the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
          - minikube update-context --profile=minikube
          - "sudo chown -R travis: /home/travis/.minikube/"
          - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
          - echo "=========================================================================================="
          - kubectl version
          - kubectl version --client #the version of the client
          - kubectl cluster-info
          - minikube status
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
                 kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
                break
              fi
              sleep 2
            done
          - echo "============================status check=============================================================="
          - minikube status
          - kubectl cluster-info
          - kubectl get pods --all-namespaces;
          - kubectl get pods -n default;
          - echo "===========================Local Gardener setup==============================================================="
          - git clone git@github.com:gardener/gardener.git && cd gardener #Local Gardener setup
          - sudo make local-garden-up #Using the nodeless cluster setup
          - sudo make local-garden-down # tear down the local Garden cluster and remove the Docker containers
          - echo "=========================================================================================="

#https://istio.io/docs/setup/platform-setup/gardener/
#https://github.com/gardener/gardener/blob/master/docs/development/local_setup.md
fleet_script_kind_gardener_tasks : &fleet_script_kind_gardener_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64
          - chmod +x ./kind
          - sudo mv ./kind /usr/local/bin/kind
          - kind get clusters #see the list of kind clusters
          - kind create cluster --name istio-testing #Create a cluster,By default, the cluster will be given the name kind
          - kind get clusters
          - sudo snap install kubectl --classic
          - kubectl config get-contexts #list the local Kubernetes contexts
          - kubectl config use-context kind-istio-testing #run following command to set the current context for kubectl
          - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml #deploy Dashboard
          - echo "===============================Waiting for Dashboard to be ready==========================================================="
          - |
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kubernetes-dashboard | grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get pod -n kubernetes-dashboard #Verify that Dashboard is deployed and running
          - kubectl create clusterrolebinding default-admin --clusterrole cluster-admin --serviceaccount=default:default #Create a ClusterRoleBinding to provide admin access to the newly created cluster
          #To login to Dashboard, you need a Bearer Token. Use the following command to store the token in a variable
          - token=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 --decode)
          - echo $token #Display the token using the echo command and copy it to use for logging into Dashboard.
          - kubectl proxy & # Access Dashboard using the kubectl command-line tool by running the following command, Starting to serve on 127.0.0.1:8001
          - |
            for i in {1..60}; do # Timeout after 1 mins, 60x1=60 secs
              if nc -z -v 127.0.0.1 8001 2>&1 | grep succeeded ; then
                break
              fi
              sleep 1
            done
          # - kind delete cluster --name istio-testing #delete the existing cluster


#https://istio.io/docs/setup/platform-setup/kind/
#https://kind.sigs.k8s.io/docs/user/quick-start/
#https://istio.io/docs/setup/getting-started/
fleet_script_kind_istio_tasks : &fleet_script_kind_istio_tasks #If you are running minikube within a VM, consider using --driver=none
      script:
          - docker version
          - curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.8.1/kind-$(uname)-amd64
          - chmod +x ./kind
          - sudo mv ./kind /usr/local/bin/kind
          - kind get clusters #see the list of kind clusters
          - kind create cluster --name istio-testing #Create a cluster,By default, the cluster will be given the name kind
          - kind get clusters
          - sudo snap install kubectl --classic
          - kubectl config get-contexts #list the local Kubernetes contexts
          - kubectl config use-context kind-istio-testing #run following command to set the current context for kubectl
          - kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml #deploy Dashboard
          - echo "===============================Waiting for Dashboard to be ready==========================================================="
          - kubectl get service --all-namespaces #list all services in all namespace
          - |
            for i in {1..60}; do # Timeout after 5 minutes, 60x2=120 secs, 2 mins
              if kubectl get pods --namespace=kubernetes-dashboard |grep Running && \
                 kubectl get pods --namespace=dashboard-metrics-scraper |grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get pod -n kubernetes-dashboard #Verify that Dashboard is deployed and running
          - kubectl create clusterrolebinding default-admin --clusterrole cluster-admin --serviceaccount=default:default #Create a ClusterRoleBinding to provide admin access to the newly created cluster
          #To login to Dashboard, you need a Bearer Token. Use the following command to store the token in a variable
          - token=$(kubectl get secrets -o jsonpath="{.items[?(@.metadata.annotations['kubernetes\.io/service-account\.name']=='default')].data.token}"|base64 --decode)
          - echo $token #Display the token using the echo command and copy it to use for logging into Dashboard.
          - sudo apt-get install net-tools -qqy #Install netcat
          - kubectl proxy & # Access Dashboard using the kubectl command-line tool by running the following command, Starting to serve on 127.0.0.1:8001
          - |
            for i in {1..60}; do # Timeout after 1 mins, 60x1=60 secs
              if nc -z -v 127.0.0.1 8001 2>&1 | grep succeeded ; then
                break
              fi
              sleep 1
            done
          - echo "===============================Install istio==========================================================="
          - 'curl -L https://istio.io/downloadIstio | sh -' #Download Istio
          -  cd istio-* #Move to the Istio package directory. For example, if the package is istio-1.6.0
          - export PATH=$PWD/bin:$PATH #Add the istioctl client to your path, The istioctl client binary in the bin/ directory.
          #precheck inspects a Kubernetes cluster for Istio install requirements
          - istioctl experimental precheck #https://istio.io/docs/reference/commands/istioctl/#istioctl-experimental-precheck
          #Begin the Istio pre-installation verification check
          # - istioctl verify-install #Error: could not load IstioOperator from cluster: the server could not find the requested resource.  Use --filename
          - istioctl version
          - istioctl manifest apply --set profile=demo #Install Istio, use the demo configuration profile
          - kubectl label namespace default istio-injection=enabled #Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later
          - kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml #Deploy the Bookinfo sample application:
          - kubectl get service --all-namespaces #list all services in all namespace
          - kubectl get services #The application will start. As each pod becomes ready, the Istio sidecar will deploy along with it.
          - kubectl get pods
          - |
            for i in {1..60}; do # Timeout after 5 minutes, 60x2=120 secs, 2 mins
              if kubectl get pods --namespace=istio-system |grep Running ; then
                break
              fi
              sleep 2
            done
          - kubectl get service --all-namespaces #list all services in all namespace
          # - |
          #   kubectl exec -it $(kubectl get pod \
          #                -l app=ratings \
          #                -o jsonpath='{.items[0].metadata.name}') \
          #                -c ratings \
          #                -- curl productpage:9080/productpage | grep -o "<title>.*</title>" <title>Simple Bookstore App</title>
          #Open the application to outside traffic
          #The Bookinfo application is deployed but not accessible from the outside. To make it accessible, you need to create an Istio Ingress Gateway, which maps a path to a route at the edge of your mesh.
          # - kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml #Associate this application with the Istio gateway
          # - istioctl analyze #Ensure that there are no issues with the configuration
          #Determining the ingress IP and ports
          #If the EXTERNAL-IP value is set, your environment has an external load balancer that you can use for the ingress gateway.
          # - kubectl get svc istio-ingressgateway -n istio-system #determine if your Kubernetes cluster is running in an environment that supports external load balancers
          # #Follow these instructions if you have determined that your environment has an external load balancer.
          # # If the EXTERNAL-IP value is <none> (or perpetually <pending>), your environment does not provide an external load balancer for the ingress gateway,access the gateway using the service’s node port.
          # - export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
          # - export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
          # - export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
          # #In certain environments, the load balancer may be exposed using a host name, instead of an IP address.
          # #the ingress gateway’s EXTERNAL-IP value will not be an IP address, but rather a host name
          # #failed to set the INGRESS_HOST environment variable, correct the INGRESS_HOST value
          # - export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          # #Follow these instructions if your environment does not have an external load balancer and choose a node port instead
          # - export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}') #Set the ingress ports
          # - export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}') #Set the ingress ports
          # - export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT #Set GATEWAY_URL
          # - echo $GATEWAY_URL #Ensure an IP address and port were successfully assigned to the environment variable
          # # - echo http://$GATEWAY_URL/productpage #Verify external access,retrieve the external address of the Bookinfo application
          # - istioctl dashboard kiali #optional dashboards installed by the demo installation,Access the Kiali dashboard. The default user name is admin and default password is admin
          # #The Istio uninstall deletes the RBAC permissions and all resources hierarchically under the istio-system namespace
          # #It is safe to ignore errors for non-existent resources because they may have been deleted hierarchically.
          # - 'istioctl manifest generate --set profile=demo | kubectl delete -f -'
          # - kubectl delete namespace istio-system #The istio-system namespace is not removed by default. If no longer needed, use the following command to remove it
          # - kubectl get virtualservices   #-- there should be no virtual services
          # - kubectl get destinationrules  #-- there should be no destination rules
          # - kubectl get gateway           #-- there should be no gateway
          # - kubectl get pods              #-- the Bookinfo pods should be deleted
          # #Bookinfo cleanup starts
          # - |
          #   SCRIPTDIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )
          #   # only ask if in interactive mode
          #   if [[ -t 0 && -z ${NAMESPACE} ]];then
          #     echo -n "namespace ? [default] "
          #     read -r NAMESPACE
          #   fi
          #   if [[ -z ${NAMESPACE} ]];then
          #     NAMESPACE=default
          #   fi
          #   echo "using NAMESPACE=${NAMESPACE}"
          #   protos=( destinationrules virtualservices gateways )
          #   for proto in "${protos[@]}"; do
          #     for resource in $(kubectl get -n ${NAMESPACE} "$proto" -o name); do
          #       kubectl delete -n ${NAMESPACE} "$resource";
          #     done
          #   done
          #   OUTPUT=$(mktemp)
          #   export OUTPUT
          #   echo "Application cleanup may take up to one minute"
          #   kubectl delete -n ${NAMESPACE} -f "$SCRIPTDIR/bookinfo.yaml" > "${OUTPUT}" 2>&1
          #   ret=$?
          #   function cleanup() {
          #     rm -f "${OUTPUT}"
          #   }
          #   trap cleanup EXIT
          #   if [[ ${ret} -eq 0 ]];then
          #     cat "${OUTPUT}"
          #   else
          #     # ignore NotFound errors
          #     OUT2=$(grep -v NotFound "${OUTPUT}")
          #     if [[ -n ${OUT2} ]];then
          #       cat "${OUTPUT}"
          #       exit ${ret}
          #     fi
          #   fi
          #   echo "Application cleanup successful"
          # - kubectl get virtualservices   #-- there should be no virtual services
          # - kubectl get destinationrules  #-- there should be no destination rules
          # - kubectl get gateway           #-- there should be no gateway
          # - kubectl get pods              #-- the Bookinfo pods should be deleted
          #Bookinfo cleanup ends
          # - echo "===============================Adding Heapster Metrics to the Kubernetes Dashboard==========================================================="
          # - sudo snap install helm --classic && helm init
          # - kubectl create serviceaccount --namespace kube-system tiller #Create a service account
          # - kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller #Bind the new service account to the cluster-admin role. This will give tiller admin access to the entire cluster
          # - kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}' #Deploy tiller and add the line serviceAccount: tiller to spec.template.spec
          # - helm install --name heapster stable/heapster --namespace kube-system #install Heapster
          - kind delete cluster --name istio-testing #delete the existing cluster

# fleet_script_minikube_docker_tasks : &fleet_script_minikube_docker_tasks #If you are running minikube within a VM, consider using --driver=none
#       script:
#           - |
#             set -eo pipefail #safety for script
#             if [[ $(egrep -c '(vmx|svm)' /proc/cpuinfo) == 0 ]]; then #check if virtualization is supported on Linux, xenial fails w 0, bionic works w 2
#                      echo "virtualization is not supported"
#             else
#                   echo "===================================="
#                   echo eval "$(egrep -c '(vmx|svm)' /proc/cpuinfo)" 2>/dev/null
#                   echo "===================================="
#                   echo "virtualization is supported"
#             fi
#           - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
#           - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
#           - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
#           - mkdir -p $HOME/.kube $HOME/.minikube
#           - touch $KUBECONFIG
#           - minikube start --profile=minikube --driver=docker --kubernetes-version=v$KUBERNETES_VERSION #the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
#           - minikube update-context --profile=minikube
#           - "sudo chown -R travis: /home/travis/.minikube/"
#           - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
#           - echo "=========================================================================================="
#           - kubectl version --client #ensure the version
#           - kubectl cluster-info
#           - minikube status
#           - echo "=========================================================================================="
#           - |
#             echo "Waiting for Kubernetes to be ready ..."
#             for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
#               if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
#                  kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
#                 break
#               fi
#               sleep 2
#             done
#           - echo "============================status check=============================================================="
#           - minikube status
#           - kubectl cluster-info
#           - kubectl get pods --all-namespaces;
#           - kubectl get pods -n default;
#           - echo "=========================================================================================="




# fleet_script_minikube_kvm_latest_tasks : &fleet_script_minikube_kvm_latest_tasks #If you are running minikube within a VM, consider using --driver=none
#       script:
#           - docker version
#           - echo "==========================Check that your CPU supports hardware virtualization================================================================"
#           - egrep -c '(vmx|svm)' /proc/cpuinfo | echo "virtualization is  supported" | echo "virtualization is not supported"
#           - egrep -c ' lm ' /proc/cpuinfo # see if your processor is 64-bit
#           - echo "===============================Installation of KVM===========================================================" #https://help.ubuntu.com/community/KVM/Installation
#           - sudo apt-get install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils
#           - sudo adduser `id -un` libvirt #Bionic (18.04 LTS) and higher
#           - sudo adduser `id -un` kvm
#           - groups #ensure that your username is added to the groups
#           - virsh list --all #Verify Installation
#           - virt-host-validate #Once configured, validate that libvirt reports no errors #https://minikube.sigs.k8s.io/docs/drivers/kvm2/
#           - echo "=========================================================================================="
#           - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
#           - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
#           - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
#           - mkdir -p $HOME/.kube $HOME/.minikube
#           - touch $KUBECONFIG
#           - sudo minikube start --profile=minikube --driver=kvm2 --kubernetes-version=v$KUBERNETES_VERSION #Start a cluster using the kvm2 driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
#           - minikube config set driver kvm2 #make kvm2 the default driver
#           - minikube update-context --profile=minikube
#           - "sudo chown -R travis: /home/travis/.minikube/"
#           - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
#           - echo "=========================================================================================="
#           - kubectl version --client #ensure the version
#           - kubectl cluster-info
#           - minikube status
#           - echo "=========================================================================================="
#           - |
#             echo "Waiting for Kubernetes to be ready ..."
#             for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
#               if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
#                  kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
#                 break
#               fi
#               sleep 2
#             done
#           - echo "============================status check=============================================================="



fleet_script_minikube_latest_tasks : &fleet_script_minikube_latest_tasks
      script:
          - |
            set -eo pipefail #safety for script
            if [[ $(egrep -c '(vmx|svm)' /proc/cpuinfo) == 0 ]]; then #check if virtualization is supported on Linux, xenial fails w 0, bionic works w 2
                     echo "virtualization is not supported"
            else
                  echo "===================================="
                  echo eval "$(egrep -c '(vmx|svm)' /proc/cpuinfo)" 2>/dev/null
                  echo "===================================="
                  echo "virtualization is supported"
            fi
          - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
          - curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
          - curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
          - mkdir -p $HOME/.kube $HOME/.minikube
          - touch $KUBECONFIG
          - sudo minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION #the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
          - minikube update-context --profile=minikube
          - "sudo chown -R travis: /home/travis/.minikube/"
          - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
          - echo "=========================================================================================="
          - kubectl version --client #ensure the version
          - kubectl cluster-info
          - minikube status
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
                 kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
                break
              fi
              sleep 2
            done
          - echo "============================status check=============================================================="
          - minikube status
          - kubectl cluster-info
          - kubectl get pods --all-namespaces;
          - kubectl get pods -n default;
          - echo "=========================================================================================="
          - echo "=============================Inspection============================================================="
          - echo "=========================================================================================="
          - kubectl get pod -o wide #The IP column will contain the internal cluster IP address for each pod.
          - kubectl get service --all-namespaces # find a Service IP,list all services in all namespaces
          - docker ps #Find the container ID or name of any container in the pod
          # - docker inspect --format '{{ .State.Pid }}' container-id-or-name #get the process ID of either container, take note of the container ID or name
          # - nsenter -t your-container-pid -n ip addr #advantage of using nsenter to run commands in a pod’s namespace – versus using something like docker exec – is that you have access to all of the commands available on the node
          # - nsenter -t your-container-pid -n ip addr #Finding a Pod’s Virtual Ethernet Interface
          # - curl $CONTAINERIP:8080 #confirm that the web server is still running on port 8080 on the container and accessible from the node
          - echo "=============================Inspecting Conntrack Connection Tracking============================================================="
          # - sudo apt-get -qq -y install conntrack #http://conntrack-tools.netfilter.org/
          - sudo apt-get -qq -y install bridge-utils # Install Linux Bridge Tools.
          - sudo apt-get -qq -y install tcpdump
          - sudo ip address show #List your networking devices
          - sudo ip netns list # list configured network namespaces
          - sudo ip netns add demo-ns #add a namespace called demo-ns
          - sudo ip netns list #see that it's in the list of available namespaces
          #A network namespace is a segregated network environment, complete with its own network stack
          # - echo "=============================start bash in our new namespace demo-ns============================================================="
          # - sudo ip netns exec demo-ns bash #start bash in our new namespace and look for interfaces that it knows about
          # - ping 8.8.8.8 #ping Google's public DNS server
          # #Observe that we have no route out of the namespace, so we don't know how to get to 8.8.8.8 from here
          # - netstat -rn #Check the routes that this namespace knows about
          # # - sudo tcpdump -ni veth0  icmp -c 4 #Confirm that the ping is still running and that both veth0 and cbr0 can see the ICMP packets in the default namespace
          # # - sudo tcpdump -ni eth0  icmp -c 4 #Now check whether eth0 can see the ICMP packets
          # # - sudo sysctl net.ipv4.conf.all.forwarding=1 #Turn forwarding on,Linux, by default, doesn't forward packets between interfaces
          # # - sudo tcpdump -ni eth0  icmp -c 4 #run tcpdump against eth0 to see if fw is working
          # # - sudo iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE #make all outgoing packets from the host look like they're coming from the host's eth0 IP address
          # # - sudo tcpdump -ni eth0  icmp # sniff
          # # - sudo conntrack -L |grep 8.8.8.8 #iptables applies new rules to new flows and leaves ongoing flows alone
          # - ip address show
          # - ip route show
          # - sudo arp #Let's understand how the connectivity looks from the namespace's layer 2 perspective. Confirm that, from demo-ns, the MAC address of192.168.255.1
          # - ping 192.168.255.1 #Attempt to to ping cbr0,From this namespace, we can only see a local loopback interface. We can no longer see or ping eth0 or cbr0.
          # - exit #Exit out of the demo-ns namespace
          # - echo "=============================Exit out of the demo-ns namespace ============================================================="
          - ip address show #Confirm that you can see the interfaces in the default namespace
          - sudo arp #Confirm that you can see the interfaces in the default namespace
          # - sudo tcpdump -ni ens4 icmp -c 4 && sleep 10 #Confirm that you can see the interfaces in the default namespace
          # - sudo conntrack -L | grep 8.8.8.8
          # - conntrack -L #list all the connections currently being tracked
          # - conntrack -E && sleep 5 #watch continuously for new connections
          # - conntrack -L -f ipv4 -d IPADDRESS -o extended #grep conntrack table information using the source IP and Port
          # - kubectl get po — all-namespaces -o wide | grep IPADDRESS #use kubectl to lookup the name of the pod using that Pod IP address
          # - conntrack -D -p tcp --orig-port-dst 80 # delete the relevant conntrack state
          # - sudo conntrack -D -s IPADDRESS
          # - conntrack -L -d IPADDRESS #list conntrack-tracked connections to a particular destination address
          - echo "=============================Inspecting Iptables Rules============================================================="
          - sysctl net.netfilter.nf_conntrack_max #sysctl setting for the maximum number of connections to track
          - sudo sysctl -w net.netfilter.nf_conntrack_max=191000 #set a new valu
          # - sudo iptables-save | head -n 20 #dump all iptables rules on a node
          - sudo iptables -t nat -L KUBE-SERVICES #list just the Kubernetes Service NAT rules
          - echo "=============================Querying Cluster DNS============================================================="
          - sudo apt install dnsutils -y #if dig is not installed
          - kubectl get service -n kube-system kube-dns #find the cluster IP of the kube-dns service CLUSTER-IP
          # - nsenter -t 14346 -n dig kubernetes.default.svc.cluster.local @IPADDRESS #nsenter to run dig in the a container namespace, Service’s full domain name of service-name.namespace.svc.cluster.local
          - sudo apt-get -qq -y install -y ipvsadm
          # - ipvsadm -Ln #list the translation table of IPs ,kube-proxy can configure IPVS to handle the translation of virtual Service IPs to pod IPs
          # - ipvsadm -Ln -t IPADDRESS:PORT #show a single Service IP
          - echo "=========================================================================================="

fleet_script_minikube_tasks : &fleet_script_minikube_tasks
      script:
          # overriding global env variables
          - |
            set -eo pipefail #safety for script
            if [[ $(egrep -c '(vmx|svm)' /proc/cpuinfo) == 0 ]]; then #check if virtualization is supported on Linux, xenial fails w 0, bionic works w 2
                     echo "virtualization is not supported"
            else
                  echo "===================================="
                  echo eval "$(egrep -c '(vmx|svm)' /proc/cpuinfo)" 2>/dev/null
                  echo "===================================="
                  echo "virtualization is supported"
            fi
          - export MINIKUBE_WANTUPDATENOTIFICATION=false
          - export MINIKUBE_WANTREPORTERRORPROMPT=false
          - export CHANGE_MINIKUBE_NONE_USER=true
          - export KUBECTL_VERSION=1.18.3
          - export KUBERNETES_VERSION=1.18.3
          - export MINIKUBE_VERSION=1.18.3
          - sudo apt-get -qq -y install conntrack #X Sorry, Kubernetes v1.18.3 requires conntrack to be installed in root's path
          - curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/v$KUBECTL_VERSION/bin/linux/amd64/kubectl && chmod +x kubectl && sudo mv kubectl /usr/local/bin/ # Download kubectl
          - curl -Lo minikube https://storage.googleapis.com/minikube/releases/v$MINIKUBE_VERSION/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/ # Download minikube
          - mkdir -p $HOME/.kube $HOME/.minikube
          - touch $KUBECONFIG
          - sudo minikube start --profile=minikube --vm-driver=none --kubernetes-version=v$KUBERNETES_VERSION #--vm-driver=none, doesn't use a VM, but containers the none driver, the kubectl config and credentials generated are owned by root in the root user’s home directory
          - minikube update-context --profile=minikube
          - "sudo chown -R travis: /home/travis/.minikube/"
          - eval "$(minikube docker-env --profile=minikube)" && export DOCKER_CLI='docker'
          - echo "=========================================================================================="
          - kubectl version --client #ensure the version
          - kubectl cluster-info
          - minikube status
          - echo "=========================================================================================="
          - |
            echo "Waiting for Kubernetes to be ready ..."
            for i in {1..150}; do # Timeout after 5 minutes, 150x2=300 secs
              if kubectl get pods --namespace=kube-system -lcomponent=kube-addon-manager|grep Running && \
                 kubectl get pods --namespace=kube-system -lk8s-app=kube-dns|grep Running ; then
                break
              fi
              sleep 2
            done
          - |
            JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}';
            until kubectl -n kube-system get pods -lk8s-app=kube-dns -o jsonpath="$JSONPATH" 2>&1 | grep -q "Ready=True"; do
              sleep 1;
              echo "waiting for kube-dns to be available";
              kubectl get pods --all-namespaces;
            done
          - kubectl run travis-example --image=redis --labels="app=travis-example" # Create example Redis deployment on Kubernetes.
          - | # Make sure created pod is scheduled and running.
            JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}';
            until kubectl -n default get pods -lapp=travis-example -o jsonpath="$JSONPATH" 2>&1 | grep -q "Ready=True"; do
              sleep 1;
              echo "waiting for travis-example deployment to be available";
              kubectl get pods -n default;
            done
          - echo "============================================="
          - kubectl get pods --all-namespaces;
          - kubectl get pods -n default;
          - echo "============================================="
fleet_script_tasks : &fleet_script_tasks
      script:
        - python --version
fleet_install_tasks : &fleet_install_tasks
      install:
        - pip install -r requirements.txt


matrix:
  fast_finish: true
  include:


# https://microk8s.io/#get-started
# https://multipass.run/docs/installing-on-linux
    - name: "multipass VM microk8s Python 3.7 on bionic" #OK
      dist: bionic
      addons:
        snaps:
          - name: multipass
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
          # - name: microk8s
          #   confinement: classic # or devmode
          #   channel: latest/stable # will be passed to --channel flag
      language: python
      python: 3.7
      before_install:
        - pip3 install virtualenv
        - virtualenv -p $(which python3) ~venvpy3
        - source ~venvpy3/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      # <<: *fleet_script_microk8s_istio_tasks
      script:
        - snap info multipass
        - sudo  multipass launch --name microk8s-vm --mem 4G --disk 40G #Launch a Multipass instance
        - sudo multipass list
        - sudo multipass exec microk8s-vm -- sudo snap install microk8s --classic --channel=1.18/stable
        - sudo multipass exec microk8s-vm -- sudo microk8s status --wait-ready
        - sudo multipass exec microk8s-vm -- sudo microk8s enable dns dashboard registry
        # - multipass launch -n bar --cloud-init cloud-config.yaml #Pass a cloud-init metadata file to an instance on launch
        # - sudo multipass shell microk8s-vm #Enter the VM instance
        # - sudo snap install microk8s --classic --channel=1.18/stable
        # - sudo microk8s status --wait-ready
#         # - sudo microk8s enable dns dashboard registry
      after_success:
        - deactivate

    - name: "multipass VM microk8s istio Python 3.7 on bionic" #OK
      dist: bionic
      addons:
        snaps:
          - name: multipass
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
          - name: microk8s
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
      language: python
      python: 3.7
      before_install:
        - pip3 install virtualenv
        - virtualenv -p $(which python3) ~venvpy3
        - source ~venvpy3/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      # <<: *fleet_script_microk8s_istio_tasks
      script:
        - snap info multipass
        # primary                 Running           192.168.64.3     Ubuntu 18.04 LTS
        # foo                     Running           192.168.64.2     Ubuntu 18.04 LTS
        - sudo  multipass launch --name foo
        # - root@192.168.64.3 #added my ~/.ssh/id_rsa.pub to the end of ~root/.ssh/authorized_keys and tested root access with ssh
        - sudo multipass list
        - sudo multipass exec foo -- free -m
        - sudo multipass exec foo -- cat /proc/meminfo
        - sudo multipass exec foo-- uname -a
        - sudo multipass exec foo -- 'curl -sfL https://get.k3s.io | sh -' #Installing k3s
        - sudo multipass exec foo -- sudo kubectl get nodes
        - sudo multipass exec foo -- cat /var/lib/rancher/k3s/server/node-token
        # - sudo multipass exec foo -- 'curl -sfL https://get.k3s.io | K3S_URL=https://192.168.64.3:6443 K3S_TOKEN=`cat token` sh -' #Adding a new node
        # - sudo multipass exec primary -- sudo kubectl get nodes
        # - sudo multipass exec primary -- sudo cat /etc/rancher/k3s/k3s.yaml #get the kubeconfig
        # - sed -i.bak -e 's/127.0.0.1:6443/192.168.64.3/g' myk3sconfig #save that locally and just change the “server” line to match our master node
        # - "!cat"
        # - export KUBECONFIG=$(pwd)/myk3sconfig
        # - kubectl get nodes
        # - kubectl api-resources --insecure-skip-tls-verify
        # - sudo multipass exec foo -- 'sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config'
        # - sudo multipass exec foo -- 'sudo chmod 644 ~/.kube/config
        # - sudo multipass exec foo -- 'kubectl get pods --all-namespaces
        # - sudo  multipass launch --name microk8s-vm --mem 4G --disk 40G #Launch a Multipass instance
        # - sudo multipass list
        # - sudo multipass exec microk8s-vm -- sudo snap install microk8s --classic --channel=1.18/stable
        # - sudo multipass exec microk8s-vm -- sudo microk8s status --wait-ready
        # - sudo multipass exec microk8s-vm -- sudo microk8s enable dns dashboard registry
      after_success:
        - deactivate









# #============================================================================ multiarch linux ============================================================================

     #https://microk8s.io/docs/setting-snap-channel
     # https://microk8s.io/docs/addons#list
     #https://multipass.run/docs/installing-on-linux
    - name: "MicroK8s Multipass edge Python 3.7 on xenial arm64"
      os: linux
      arch: arm64
      dist: xenial
      addons:
        # apt:
        #   sources:
        #     - deadsnakes
        #     - sourceline: 'ppa:ubuntu-toolchain-r/test'
        #     - sourceline: 'deb https://packagecloud.io/chef/stable/ubuntu/precise main'
        #       key_url: 'https://packagecloud.io/gpg.key'
        #   packages:
        #   - cmake
        #   - time
        snaps:
          - name: multipass
            confinement: classic # or devmode
            channel: latest/edge # will be passed to --channel flag
          - name: microk8s
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
      language: python
      python: 3.7
      # env:
      #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
      # compiler:
      #  - gcc
      #  - clang
      before_install:
        - pip3 install virtualenv
        - virtualenv -p $(which python3) ~venvpy3
        - source ~venvpy3/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      script:
        # - snap install multipass --beta #For architectures other than amd64, you’ll need the beta channel at the moment
        # - sudo snap install multipass --edge --beta # use the edge channel to get the latest development build
        # - sudo snap install microk8s --classic --channel=edge
        - snap info multipass
        - snap info microk8s #All the currently available channels
        - sudo microk8s stop
        - sudo rm -rf /var/snap/microk8s/common/var/lib/containerd
        - sudo ls -l /var/snap/microk8s/current/args/containerd-template.toml
        - sudo sed 's%snapshotter = "overlayfs"%snapshotter = "zfs"%g' /var/snap/microk8s/current/args/containerd-template.toml
        # - sudo zfs create -o mountpoint=/var/snap/microk8s/common/var/lib/containerd/io.containerd.snapshotter.v1.zfs $POOL/containerd
        - sudo microk8s start
        - sudo microk8s status

      after_success:
        - deactivate

     #https://multipass.run/docs/installing-on-linux
    - name: "MicroK8s Multipass beta Python 3.7 on xenial arm64"
      os: linux
      arch: arm64
      dist: xenial
      addons:
        snaps:
          - name: multipass
            confinement: classic # or devmode
            channel: latest/beta # will be passed to --channel flag
          - name: microk8s
            confinement: classic # or devmode
            channel: latest/stable # will be passed to --channel flag
      language: python
      python: 3.7
      # env:
      #   - LIB_PATH="/usr/bin/shared/x86_64/v1"
      # compiler:
      #  - gcc
      #  - clang
      before_install:
        - pip3 install virtualenv
        - virtualenv -p $(which python3) ~venvpy3
        - source ~venvpy3/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      script:
        # - sudo snap install multipass --beta #For architectures other than amd64, you’ll need the beta channel at the moment
        # - snap install multipass --edge --beta # use the edge channel to get the latest development build
        - multipass version
        - sudo microk8s status
        - snap info multipass
        - snap info microk8s #All the currently available channels
        - sudo microk8s stop
        - sudo rm -rf /var/snap/microk8s/common/var/lib/containerd
        - sudo ls -l /var/snap/microk8s/current/args/containerd-template.toml
        - sudo sed 's%snapshotter = "overlayfs"%snapshotter = "zfs"%g' /var/snap/microk8s/current/args/containerd-template.toml
        - sudo zfs create -o mountpoint=/var/snap/microk8s/common/var/lib/containerd/io.containerd.snapshotter.v1.zfs $POOL/containerd
        - sudo microk8s start
        - sudo microk8s status

      after_success:
        - deactivate


#   # =============================================macOS=============================================


    #The default backend on macOS is hyperkit, wrapping Apple’s Hypervisor.framework
    #You need macOS Yosemite, version 10.10.3 or later installed on a 2010 or newer Mac
    #https://multipass.run/docs/installing-on-macos
    #https://docs.travis-ci.com/user/reference/osx/#macos-version
    - name: "microk8s Multipass Python 2.7.17 on macOS 10.15.4 osx xcode11.5" #installer: Error - Your CPU does not have the features necessary for Multipass. Installation cannot proceed.
      os: osx
      osx_image: xcode11.5
      language: shell
      addons:
        homebrew:
          packages:
          - beanstalk
          update: true
      # addons:
      #   homebrew:
      #     # brewfile: Brewfile.travis
      #     # packages:
      #     #   - multipass
      #     # taps: homebrew/cask-versions
      #     casks: # Installing Casks
      #       - multipass
      #     update: true
      before_install:
        - pip install virtualenv
        - virtualenv -p $(which python2) ~venvpy2
        - source ~venvpy2/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      script:
        # - /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)" #Install Homebrew
        # - brew cask install multipass
        # - multipass version
        - brew list --versions
        # - sudo multipass set local.driver=virtualbox #If you’d like to use VirtualBox, run the following in the terminal after installing Multipass
      after_success:
        # - brew cask uninstall multipass
        - deactivate

    - name: "microk8s Multipass cluster1 Python 2.7.17 on macOS 10.15.4 osx xcode11.5" #installer: Error - Your CPU does not have the features necessary for Multipass. Installation cannot proceed.
      os: osx
      osx_image: xcode11.5
      language: shell
      addons:
        homebrew:
          casks: #Installing Casks
          - dotnet-sdk
      # addons:
      #   homebrew:
      #     # brewfile: Brewfile.travis
      #     # packages:
      #     #   - multipass
      #     # taps: homebrew/cask-versions
      #     casks: # Installing Casks
      #       - multipass
      #     update: true
      before_install:
        - pip install virtualenv
        - virtualenv -p $(which python2) ~venvpy2
        - source ~venvpy2/bin/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      script:
        # - /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)" #Install Homebrew
        # - brew cask install multipass
        # - multipass version
        # - sh-keygen -b 2048 -f ~/.ssh/multipass -t rsa -q -N "" #create a ssh key
        # - cat ~/.ssh/multipass.pub >> cloud-init.yaml #provide the ssh public key to cloud-init file in order to ssh into multipass vm with ssh private key
        # - sudo multipass set local.driver=virtualbox #If you’d like to use VirtualBox, run the following in the terminal after installing Multipass
        - brew list --versions
      after_success:
        - brew cask uninstall multipass
        - deactivate




    # - name: "Multipass microk8s cluster2 Python 2.7.17 on macOS 10.15.4 osx xcode11.5" #installer: Error - Your CPU does not have the features necessary for Multipass. Installation cannot proceed.
    #   os: osx
    #   osx_image: xcode11.5
    #   language: shell
    #   addons:
    #     homebrew:
    #       casks: #Installing Casks
    #       - dotnet-sdk
    #   # addons:
    #   #   homebrew:
    #   #     # brewfile: Brewfile.travis
    #   #     # packages:
    #   #     #   - multipass
    #   #     # taps: homebrew/cask-versions
    #   #     casks: # Installing Casks
    #   #       - multipass
    #   #     update: true
    #   before_install:
    #     - pip install virtualenv
    #     - virtualenv -p $(which python2) ~venvpy2
    #     - source ~venvpy2/bin/activate
    #   <<: *fleet_install_tasks
    #   <<: *fleet_script_tasks
    #   script:
    #     # - /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)" #Install Homebrew
    #     - brew cask install multipass
    #     - multipass version
    #     - multipass find #returns a list of the available images
    #     - sudo multipass launch ubuntu
    #     - sudo multipass launch 16.04
    #     - multipass list #list instances
    #     # - multipass shell rewarded-stud #connect to a running instance,find its instance name with the list option
    #     #run commands in an instance indirectly without connecting directly to the instance with the exec option, and the command separated with a double dash (--)
    #     # install MicroK8s as a snap on your Multipass-hosted Ubuntu instance
    #     # - multipass exec rewarded-stud -- ls -a
    #     # - multipass exec rewarded-stud -- sudo snap install microk8s --classic #Alternatively,without connecting to the instance, install MicroK8
    #     # - multipass exec rewarded-stud -- sudo microk8s.start  #Alternatively,without connecting to the instance,start MicroK8
    #     # - multipass exec rewarded-stud -- /snap/bin/microk8s.start #Alternatively,without connecting to the instance,start MicroK8
    #     # - multipass exec rewarded-stud -- sudo usermod -a -G microk8s multipass #Alternatively,without connecting to the instance, add the multipass user to the microk8s group, so it has permissions to run and access the services needed
    #     # - multipass exec intrigued-kudu -- /snap/bin/microk8s.kubectl cluster-info #Alternatively,without connecting to the instance,see what services are running on the Kubernetes cluster,Replace “127.0.0.1” with the IP of the VM, and you should be able to access services as usual
    #     # - multipass exec intrigued-kudu -- /snap/bin/microk8s.config > kubeconfig #Alternatively,without connecting to the instance,copy the kubeconfig file from the instance to the host multipass installed
    #     # - kubectl --kubeconfig=kubeconfig get all --all-namespaces #use the new config to access Kubernetes running on the instance
    #     # - sudo multipass stop rewarded-stud
    #     # - sudo multipass delete rewarded-stud
    #     # - sudo multipass purge #to completely remove all deleted instances and images
    #     # - sh-keygen -b 2048 -f ~/.ssh/multipass -t rsa -q -N "" #create a ssh key
    #     # - cat ~/.ssh/multipass.pub >> cloud-init.yaml #provide the ssh public key to cloud-init file in order to ssh into multipass vm with ssh private key
    #     # - sudo multipass set local.driver=virtualbox #If you’d like to use VirtualBox, run the following in the terminal after installing Multipass
    #     - brew list --versions
      # after_success:
      #   - brew cask uninstall multipass
      #   - deactivate



#   # =============================================windows=============================================

    #Only Windows 10 Pro or Enterprise, version 1803 (“April 2018 Update”) or later is currently supported.
    #You will need either Hyper-V enabled (only Windows 10 Professional or Enterprise), or VirtualBox installed.
    #Multipass defaults to using Hyper-V as it’s virtualization provider
    #https://multipass.run/docs/installing-on-windows
    - name: "microk8s Multipass  Python 3.8 on Windows"
      os: windows
      language: shell
      env:
        - PATH=/c/Python38:/c/Python38/Scripts:$PATH
      before_install:
        - choco install python --version 3.8.1
        - pip install virtualenv
        - virtualenv $HOME/venv
        - source $HOME/venv/Scripts/activate
      <<: *fleet_install_tasks
      <<: *fleet_script_tasks
      script:
          - choco install wget
          - wget https://github.com/canonical/multipass/releases/download/v1.2.1/multipass-1.2.1+win-win64.exe
          # - multipass set local.driver=virtualbox
          # - multipass version
      after_success:
        - deactivate
